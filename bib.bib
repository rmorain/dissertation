@INPROCEEDINGS{morain2022symbolic,
  author={Morain, Robert and Vargas, Kenneth and Ventura, Dan},
  booktitle={IEEE International Conference on Machine Learning and Applications (ICMLA)}, 
  title={Symbolic Semantic Memory in Transformer Language Models}, 
  year={2022},
  volume={},
  number={},
  pages={992-998},
  keywords={Filtering;Knowledge based systems;Semantics;Machine learning;Transformers;Data models;Data mining;language modeling;knowledge integration;natural language processing},
  doi={10.1109/ICMLA55696.2022.00166}}

@book{tulving1972episodic,
	author={Endel Tulving},
	year={1972},
	title={Episodic and Semantic Memory},
	publisher={Academic Press},
	address={Oxford, England},
	abstract={In this chapter I discuss the possibility that semantic memory, among other things, is not the kind of memory that psychologists have been studying since the time of Ebbinghaus. I will suggest that there are sufficiently fundamental differences between the two forms of memory to recommend that we consider, at least for the time being, the two categories separately. To facilitate subsequent discussion, I will refer to this other kind of memory, the one that semantic memory is not, as 'episodic' memory. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}
}

@inproceedings{
mao2019neuro,
title={The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision},
author={Jiayuan Mao and Chuang Gan and Pushmeet Kohli and Joshua B. Tenenbaum and Jiajun Wu},
booktitle={Proceedings of the International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJgMlhRctm},
}

@inproceedings{johnson2017clevr,
  title={{CLEVR}: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning},
  author={Johnson, Justin and Hariharan, Bharath and van der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C and Girshick, Ross},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2901--2910},
  year={2017}
}

@inproceedings{devlin2018bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    volume = {1},
    year = {2019},
    pages = "4171--4186",
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI Blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@book{collins1972experiments,
	author={Allan M. Collins and M. R. Quillian},
	year={1972},
	title={Experiments on Semantic Memory and Language Comprehension},
	publisher={John Wiley \& Sons},
	address={Oxford, England},

	abstract={Describes a series of reaction-time experiments testing the applicability of a computer model to human semantic memory and language comprehension. The program comprehends a written text by relating it to memory structures in a semantic network of factual information. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}
}

@book{laird2012soar,
  title={The Soar Cognitive Architecture},
  author={Laird, John E},
  year={2012},
  publisher={MIT press}
}

@InProceedings{perez2018film,
  title={Fi{LM}: Visual Reasoning with a General Conditioning Layer},
  author={Ethan Perez and Florian Strub and Harm de Vries and Vincent Dumoulin and Aaron C. Courville},
  booktitle={AAAI Conference on Artificial Intelligence},
  pages={3942-3951},
  year={2018}
}

@inproceedings{chen2019uniter,
  author    = {Yen{-}Chun Chen and
               Linjie Li and
               Licheng Yu and
               Ahmed El Kholy and
               Faisal Ahmed and
               Zhe Gan and
               Yu Cheng and
               Jingjing Liu},
  title     = {{UNITER:} UNiversal Image-TExt Representation Learning},
  booktitle = {Computer Vision - {ECCV} 2020 - 16th European Conference, Glasgow,
               UK, August 23-28, 2020, Proceedings, Part {XXX}},
  series    = {Lecture Notes in Computer Science},
  volume    = {12375},
  pages     = {104--120},
  publisher = {Springer},
  year      = {2020},
  url       = {https://doi.org/10.1007/978-3-030-58577-8\_7},
  doi       = {10.1007/978-3-030-58577-8\_7},
  timestamp = {Fri, 25 Sep 2020 09:25:45 +0200},
  biburl    = {https://dblp.org/rec/conf/eccv/ChenLYK0G0020.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{bengio2009curriculum,
  title={Curriculum Learning},
  author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 26th Annual International Conference on Machine Learning},
  pages={41--48},
  year={2009}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5998--6008},
  year={2017}
}

@inproceedings{bosselut2019comet,
  author    = {Antoine Bosselut and
              Hannah Rashkin and
              Maarten Sap and
              Chaitanya Malaviya and
              Asli {\c{C}}elikyilmaz and
              Yejin Choi},
  title     = {{COMET}: Commonsense Transformers for Automatic Knowledge Graph Construction},
  booktitle = {Proceedings of the Conference of the Association for Computational Linguistics},
  volume    = {1},
  pages     = {4762-4779},
  year      = {2019},
}

@inproceedings{sap2019atomic,
  title={ATOMIC: An atlas of machine commonsense for if-then reasoning},
  author={Sap, Maarten and Le Bras, Ronan and Allaway, Emily and Bhagavatula, Chandra and Lourie, Nicholas and Rashkin, Hannah and Roof, Brendan and Smith, Noah A and Choi, Yejin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={3027--3035},
  year={2019}
}

@inproceedings{liu2004conceptnet,
  author    = {Robyn Speer and
               Joshua Chin and
               Catherine Havasi},
  title     = {ConceptNet 5.5: An Open Multilingual Graph of General Knowledge},
  booktitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
  pages     = {4444-4451},
  year      = {2017},
}

@inproceedings{bosselut2019dynamic,
  author    = {Antoine Bosselut and
               Ronan Le Bras and
               Yejin Choi},
  title     = {Dynamic Neuro-Symbolic Knowledge Graph Construction for Zero-shot
               Commonsense Question Answering},
  booktitle = {Thirty-Fifth {AAAI} Conference on Artificial Intelligence, {AAAI}
               2021, Thirty-Third Conference on Innovative Applications of Artificial
               Intelligence, {IAAI} 2021, The Eleventh Symposium on Educational Advances
               in Artificial Intelligence, {EAAI} 2021, Virtual Event, February 2-9,
               2021},
  pages     = {4923--4931},
  publisher = {{AAAI} Press},
  year      = {2021},
  url       = {https://ojs.aaai.org/index.php/AAAI/article/view/16625},
  timestamp = {Wed, 02 Jun 2021 18:09:11 +0200},
  biburl    = {https://dblp.org/rec/conf/aaai/BosselutBC21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sap2019socialiqa,
  author    = {Maarten Sap and
               Hannah Rashkin and
               Derek Chen and
               Ronan Le Bras and
               Yejin Choi},
  title     = {Social {IQ}a: Commonsense Reasoning about Social Interactions},
  booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing},
  pages     = {4462--4472},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
}

@inproceedings{
lample2019deep,
title={Deep Learning For Symbolic Mathematics},
author={Guillaume Lample and Fran√ßois Charton},
booktitle={Proceedings of the International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=S1eZYeHFDS}
}

@article{wu2020comprehensive,
  title={A comprehensive survey on graph neural networks},
  author={Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Philip, S Yu},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={Early access},
  year={2020},
  publisher={IEEE},
}

@inproceedings{wang2018glue,
  title={{GLUE}: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  booktitle={Proceedings of the International Conference on Learning Representations},
  year={2019},
}

@inproceedings{wang2019superglue,
  title={Super{GLUE}: A stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={3261--3275},
  year={2019}
}

@inproceedings{gan2017vqs,
  title={{VQS}: Linking segmentations to questions and answers for supervised attention in {VQA} and question-focused semantic segmentation},
  author={Gan, Chuang and Li, Yandong and Li, Haoxiang and Sun, Chen and Gong, Boqing},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1811--1820},
  year={2017}
}

@inproceedings{zellers2019recognition,
  title={From recognition to cognition: Visual commonsense reasoning},
  author={Zellers, Rowan and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6720--6731},
  year={2019}
}

@inproceedings{rashkin2018modeling,
    title = "Modeling Naive Psychology of Characters in Simple Commonsense Stories",
    author = "Rashkin, Hannah  and
      Bosselut, Antoine  and
      Sap, Maarten  and
      Knight, Kevin  and
      Choi, Yejin",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
    year = {2018},
    publisher = "Association for Computational Linguistics",
    pages = "2289--2299",
}

@inproceedings{zhang2017weisfeiler,
  title={Weisfeiler-{L}ehman neural machine for link prediction},
  author={Zhang, Muhan and Chen, Yixin},
  booktitle={Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={575--583},
  year={2017}
}

@incollection{mikolov2013word2vec,
title = {Distributed Representations of Words and Phrases and their Compositionality},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
booktitle = {Advances in Neural Information Processing Systems},
volume = {26},
pages = {3111--3119},
year = {2013},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf}
}

@article{Ahn2016neural,
author = {Ahn, Sungjin and Choi, Heeyoul and P{\"{a}}rnamaa, Tanel and Bengio, Yoshua},
  title     = {A Neural Knowledge Language Model},
  journal   = {CoRR},
  volume    = {abs/1608.00318},
  year      = {2016},
  url       = {http://arxiv.org/abs/1608.00318},
  archivePrefix = {arXiv},
  eprint    = {1608.00318},
  timestamp = {Mon, 13 Aug 2018 16:47:35 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/AhnCPB16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{choudhury2017nous,
  author={S. {Choudhury} and K. {Agarwal} and S. {Purohit} and B. {Zhang} and M. {Pirrung} and W. {Smith} and M. {Thomas}},
  booktitle={Proceedings of the IEEE 33rd International Conference on Data Engineering}, 
  title={NOUS: Construction and Querying of Dynamic Knowledge Graphs}, 
  year={2017},
  volume={},
  number={},
  pages={1563-1565},}
 

@article{vrandecic2014wikidata,
    author={Denny Vrande},
    year={2014},
    title={Wikidata: a free collaborative knowledgebase},
    journal={Communications of the ACM},
    volume={57},
    number={10},
    pages={78-85}
}

@article{cafarella2011structureddata,
	author={Michael J. Cafarella and Alon Halevy and Jayant Madhavan},
	year={2011},
	title={Structured data on the web},
	journal={Communications of the ACM},
	volume={54},
	number={2},
	pages={72-79}
}

@article{miller1998rdf,
	author={Eric Miller},
	year={1998},
	title={An introduction to the resource description framework},
	journal={Bulletin of the American Society for Information Science and Technology},
	volume={25},
	number={1},
	pages={15-19}
}

@book{kahneman2011thinking,
  title={Thinking, Fast and Slow},
  author={Kahneman, Daniel},
  year={2011},
  publisher={Macmillan}
}
  
@inproceedings{brown2020language,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1877--1901},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{zhang2021differentiable,
  title={Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners},
  author={Zhang, Ningyu and Li, Luoqiu and Chen, Xiang and Deng, Shumin and Bi, Zhen and Tan, Chuanqi and Huang, Fei and Chen, Huajun},
  journal={arXiv preprint arXiv:2108.13161},
  year={2021}
}

@inproceedings{bender2021dangers,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the ACM Conference on Fairness, Accountability, and Transparency},
  pages={610--623},
  year={2021}
}

@incollection{jones2015models,
  title={Models of Semantic Memory},
  author={Jones, Michael N and Willits, Jon and Dennis, Simon and Jones, Michael},
  booktitle={Oxford Handbook of Mathematical and Computational Psychology},
  pages={232--254},
  year={2015},
  publisher={Oxford University Press},
  address = {New York}
}

@inproceedings{johnson2017inferring,
  title={Inferring and executing programs for visual reasoning},
  author={Johnson, Justin and Hariharan, Bharath and Van Der Maaten, Laurens and Hoffman, Judy and Fei-Fei, Li and Lawrence Zitnick, C and Girshick, Ross},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={2989--2998},
  year={2017}
}

@article{bordes2015large,
  author    = {Antoine Bordes and
               Nicolas Usunier and
               Sumit Chopra and
               Jason Weston},
  title     = {Large-scale Simple Question Answering with Memory Networks},
  journal   = {CoRR},
  volume    = {abs/1506.02075},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.02075},
  eprinttype = {arXiv},
  eprint    = {1506.02075},
  timestamp = {Mon, 13 Aug 2018 16:48:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BordesUCW15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{merity2016pointer,
  author    = {Stephen Merity and
               Caiming Xiong and
               James Bradbury and
               Richard Socher},
  title     = {Pointer Sentinel Mixture Models},
  booktitle = {Proceedings of the International Conference on Learning Representations},
  year      = {2017}
}

@article{wolf2019huggingface,
  author    = {Thomas Wolf and
               Lysandre Debut and
               Victor Sanh and
               Julien Chaumond and
               Clement Delangue and
               Anthony Moi and
               Pierric Cistac and
               Tim Rault and
               R{\'{e}}mi Louf and
               Morgan Funtowicz and
               Jamie Brew},
  title     = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  journal   = {CoRR},
  volume    = {abs/1910.03771},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.03771},
  eprinttype = {arXiv},
  eprint    = {1910.03771},
  timestamp = {Tue, 02 Jun 2020 12:49:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-03771.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{miller2016key,
  author    = {Alexander H. Miller and
               Adam Fisch and
               Jesse Dodge and
               Amir{-}Hossein Karimi and
               Antoine Bordes and
               Jason Weston},
  title     = {Key-Value Memory Networks for Directly Reading Documents},
  booktitle = {Proceedings of the Conference on Empirical Methods in Natural
               Language Processing},
  pages     = {1400--1409},
  year      = {2016}
 }

@inproceedings{kingma2015adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {International Conference on Learning Representations},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{guu2020realm,
  title={Realm: Retrieval-augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
  journal={arXiv preprint arXiv:2002.08909},
  year={2020}
}

@article{wang2021kepler,
  title={KEPLER: A unified model for knowledge embedding and pre-trained language representation},
  author={Wang, Xiaozhi and Gao, Tianyu and Zhu, Zhaocheng and Zhang, Zhengyan and Liu, Zhiyuan and Li, Juanzi and Tang, Jian},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={176--194},
  year={2021},
  publisher={MIT Press}
}

@inproceedings{liu2019knowledge,
  author    = {Angli Liu and
               Jingfei Du and
               Veselin Stoyanov},
  title     = {Knowledge-Augmented Language Model and Its Application to Unsupervised
               Named-Entity Recognition},
  booktitle = {Proceedings of the Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies},
  pages     = {1142--1150},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/n19-1117},
  doi       = {10.18653/v1/n19-1117},
  timestamp = {Fri, 06 Aug 2021 00:41:31 +0200},
  biburl    = {https://dblp.org/rec/conf/naacl/LiuDS19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{peters2019knowledge,
  author    = {Matthew E. Peters and
               Mark Neumann and
               Robert L. Logan IV and
               Roy Schwartz and
               Vidur Joshi and
               Sameer Singh and
               Noah A. Smith},
  title     = {Knowledge Enhanced Contextual Word Representations},
  booktitle = {Proceedings of the Conference on Empirical Methods in Natural
               Language Processing and the International Joint Conference on
               Natural Language Processing},
  pages     = {43--54},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  doi       = {10.18653/v1/D19-1005},
  timestamp = {Thu, 07 Apr 2022 09:14:07 +0200},
  biburl    = {https://dblp.org/rec/conf/emnlp/PetersNLSJSS19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{chatGPT,
title={Introducing {C}hat{GPT}},
author={Open{AI}},
howpublished={\url{https://openai.com/blog/chatgpt}},
note={Accessed 2023-4-11},
year={2023}
}

@article{perez2001mexica,
author = {Rafael P{\'e}rez {Y} P{\'e}rez and Mike Sharples},
journal = {Journal of Experimental \& Theoretical Artificial Intelligence},
number = {2},
pages = {119-139},
publisher = {Taylor & Francis},
title = {{MEXICA}: A computer model of a cognitive account of creative writing},
volume = {13},
year = {2001},
bdsk-url-1 = {https://doi.org/10.1080/09528130010029820}
}

@inproceedings{boggia2022poetry,
  author = {Michele Boggia and Sardana Ivanova and Simo Linkola and Anna Kantosalo and Hannu Toivonen},
  title = {One Line at a Time ‚Äî Generation and Internal Evaluation of Interactive Poetry},
  booktitle = {Proceedings of the International Conference on Computational Creativity},
  year = {2022},
  publisher={Association for Computational Creativity},
  pages={7--11},
  }

  @techreport{ritchie2003jape,
  author={Graeme Ritchie},
  title={The {JAPE} riddle generator: technical specification},
  institution={School of Informatics, University of Edinburgh},
  number={EDI-INF-RR-0158},
  year={2003}
  }

  @misc{radford2018gpt2,
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  description = {Language Models are Unsupervised Multitask Learners},
  title = {Language Models are Unsupervised Multitask Learners},
  year = 2018,
  howpublished={\url{https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}},
  note={Accessed 2023-4-11},
}

@inproceedings{ouyang2022instructgpt,
 author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F and Leike, Jan and Lowe, Ryan},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {27730--27744},
 publisher = {Curran Associates, Inc.},
 title = {Training language models to follow instructions with human feedback},
 volume = {35},
 year = {2022}
}

@inproceedings{tyler2020puns,
  author = {Bradley Tyler and Katherine Wilsdon and Paul Bodily},
  title = {Computational Humor: Automated Pun Generation},
  pages = {181--184},
  booktitle = {Proceedings of the International Conference on Computational Creativity},
  year = {2020},
  isbn = {978-989-54160-2-8},
  publisher = {Association for Computational Creativity},
  url = {http://computationalcreativity.net/iccc20/papers/152-iccc20.pdf}
}

@inproceedings{hamalainen2019movie,
  author = {Mika H√§m√§l√§inen and Khalid Alnajjar},
  title = {Modelling the Socialization of Creative Agents in a Master-Apprentice Setting: The Case of Movie Title Puns},
  pages = {266--273},
  booktitle = {Proceedings of the International Conference on Computational Creativity},
  year = {2019},
  isbn = {978-989-54160-1-1},
  publisher = {Association for Computational Creativity},
  url = {http://computationalcreativity.net/iccc2019/papers/iccc19-paper-12.pdf}
}

@inproceedings{spendlove2018micros,
    Author = {Brad Spendlove and Nathan Zabriskie and Dan Ventura},
    Booktitle = {Proceedings of the International Conference on Computational Creativity},
    Pages = {161--168},
    Publisher = {Association for Computational Creativity},
    Title = {An {HBPL}-based Approach to the Creation of Six-word Stories},
    Year = {2018}
}

@misc{chiengenerating,
  title={Generating Six-Word Stories},
  author={Chien, Gianna},
  howpublished={\url{http://cs230.stanford.edu/projects_fall_2020/reports/55790134.pdf}},
  year={2020}
}

@inproceedings{spendlove2020hieros,
  author = {Brad Spendlove and Dan Ventura},
  title = {Creating Six-word Stories via Inferred Linguistic and Semantic Formats},
  pages = {123--130},
  booktitle = {Proceedings of the International Conference on Computational Creativity},
  year = {2020},
  publisher = {Association for Computational Creativity},
}

@inproceedings{lamb2019poetry,
  author = {Carolyn Lamb and Daniel G. Brown},
  title = {TwitSong 3.0: Towards Semantic Revisions in Computational Poetry},
  pages = {212--219},
  booktitle = {Proceedings of the International Conference on Computational Creativity},
  year = {2019},
  isbn = {978-989-54160-1-1},
  publisher = {Association for Computational Creativity},
  url = {http://computationalcreativity.net/iccc2019/papers/iccc19-paper-9.pdf}
}

@inproceedings{oliveira2016poetryme,
    author = {Oliveira, Hugo G. and Alves, Ana O.},
    booktitle = {Proceedings of the International Conference on Computational Creativity},
    citeulike-article-id = {14553624},
    citeulike-linkout-0 = {http://www.computationalcreativity.net/iccc2016/wp-content/uploads/2016/01/Poetry-from-Concept-Maps.pdf},
    pages = {246--253},
    posted-date = {2018-03-21 13:02:12},
    priority = {2},
    publisher = {Sony Computer Science Laboratories},
    title = {Poetry from Concept Maps--Yet Another Adaptation of {PoeTryMe's} Flexible Architecture},
    url = {http://www.computationalcreativity.net/iccc2016/wp-content/uploads/2016/01/Poetry-from-Concept-Maps.pdf},
    year = {2016}
}

@inproceedings{concepcion2019ines,
  author = {Eugenio Concepci√≥n and Pablo Gerv√°s and Gonzalo M√©ndez},
  title = {Evolving the {INES} Story Generation System: From Single to Multiple Plot Lines},
  pages = {220-227},
  booktitle = {Proceedings of the International Conference on Computational Creativity},
  year = {2019},
  isbn = {978-989-54160-1-1},
  publisher = {Association for Computational Creativity},
  url = {http://computationalcreativity.net/iccc2019/papers/iccc19-paper-23.pdf}
}

@inproceedings{Pickering2017,
    author = {Pickering, Todd and Jordanous, Anna},
    booktitle = {Proceedings of the International Conference on Computational Creativity},
    citeulike-article-id = {14553674},
    citeulike-linkout-0 = {http://computationalcreativity.net/iccc2017/ICCC_17_accepted_submissions/ICCC-17_paper_39.pdf},
    pages = {213--220},
    posted-date = {2018-03-21 13:05:56},
    priority = {2},
    publisher = {Association for Computational Creativity},
    title = {Applying Narrative Theory to Aid Unexpectedness in a Story Generation System},
    year = {2017}
}


@InProceedings{ramesh2021dalle,
  title = 	 {Zero-Shot Text-to-Image Generation},
  author =       {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the International Conference on Machine Learning},
  pages = 	 {8821--8831},
  year = 	 {2021},
  volume = 	 {139},
  pdf = 	 {http://proceedings.mlr.press/v139/ramesh21a/ramesh21a.pdf},
  abstract = 	 {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.}
}

@book{boden1990creativity,
    title = {The creative mind},
    publisher = {Abacus},
    year = {1990},
    author = {Margaret Boden}
}

@article{wiggins2006creativity,
  author    = {Geraint A. Wiggins},
  title     = {A preliminary framework for description, analysis and comparison of
               creative systems},
  journal   = {Knowledge Based Systems},
  volume    = {19},
  number    = {7},
  pages     = {449--458},
  year      = {2006},
  url       = {https://doi.org/10.1016/j.knosys.2006.04.009},
  doi       = {10.1016/j.knosys.2006.04.009},
  timestamp = {Tue, 25 Feb 2020 09:02:09 +0100},
  biburl    = {https://dblp.org/rec/journals/kbs/Wiggins06.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{krippendorff2013contentanalysis,
    author = {Klaus Krippendorff},
    publisher = {Sage},
    title = {Content Analysis: An Introduction to Its Methodology},
    year = {2013},
    edition = {3rd},
    pages={221--250}
}


@article{ritchie07,
 author  = {Graeme Ritchie},
 title = {Some Empirical Criteria for Attributing Creativity to a Computer Program},
 journal = {Minds and Machines},
 publisher = {Springer},
 year = {2007}, 
 pages = {76-99},
 volume = {17},
}


@misc{UCI,
 author = "A. Asuncion and D.J. Newman",
 title = "{UCI} Machine Learning Repository", 
 note = "http://www.ics.uci.edu/~mlearn/MLRepository.html",
 howpublished = "University of California, Irvine, School of Information and Computer Sciences"
}


@inproceedings{veale07,
 author = {Tony Veale and Yanfen Hao},
 title = {Comprehending and Generating Apt Metaphors: A Web-driven, Case-based Approach to Figurative Language},
 booktitle = {Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence (AAAI-07)},
 year = {2007},
 pages = {1471--1476},
 publisher = {AAAI Press},
 address = {Vancouver, British Columbia},
 }

@article{lyu04,
 author = {Siwei Lyu and Daniel Rockmore and Hany Farid},
 title = {A Digital Technique for Art Authentication},
 journal = {Proceedings of the National Academy of Sciences},
 volume = {101},
 number = {49},
 year = {2004},
 pages = {17006--17010},
 }

@incollection{Woods81,
 author =      "Woods, W.A.",
 title =       "Procedural semantics as a theory of meaning",
 booktitle =   "Elements of Discourse Understanding",
 publisher =   "Cambridge University Press",
 address =     "Cambridge, UK",
 year =        1981,
 pages =       "300--334"
}

@Book{Ruch07,
 title =       {The Sense of Humor: Explorations of a Personality
Characteristic},
 publisher =   {Mouton de Gruyter},
 year =        2007,
 series =      {Mouton Select},
 address =     {Berlin}
}

@techreport{OZ,
 AUTHOR =      {Mark Kantrowitz},
 TITLE =       {Natural Language Text Generation in the {OZ}
                 Interactive Fiction Project},
 YEAR =        {1990},
 INSTITUTION =         {School of Computer Science, Carnegie Mellon
                 University},
 ADDRESS =     {Pittsburgh, PA},
 NUMBER =      {CMU-CS-90-158},
 TYPE =        {Technical Report}
}

@inproceedings{grace2014expect,
  booktitle = {Proceedings of the International Conference on Computational
               Creativity},
  year={2014},
  title={What to expect when you're expecting: The role of unexpectedness in computationally evaluating creativity.},
  author={Grace, Kazjon and Maher, Mary Lou},
  publisher={Association for Computational Creativity},
  pages={120--128},
}

@inproceedings{spendlove2022competitive,
  title={Competitive Language Games as Creative Tasks with Well-Defined Goals.},
  year={2022},
  author={Spendlove, Brad and Ventura, Dan},
  booktitle = {Proceedings of the International Conference on Computational Creativity},
  publisher={Association for Computational Creativity},
  pages={291--299},
}

@misc{codenames2015,
    author = {Chv√°til, Vladim√≠r},
    year = {2015},
    title = {Codenames},
    address = {Kladno, Czech Republic},
    publisher = {Czech Games Edition},
    type = {Board Game}
}

@inproceedings{ventura2017howto,
  title = {How to Build a {CC} System},
  author = {Ventura, Dan},
  booktitle = {Proceedings of the 8th International Conference on Computational Creativity},
  year = {2017},
pages = {253--260},
  publisher={Association for Computational Creativity},
}

@inproceedings{pease2011face,
author = {Pease, Alison and Colton, Simon},
year = {2011},
pages = {15--22},
title = {On Impact and Evaluation in Computational Creativity: A Discussion of the {T}uring Test and an Alternative Proposal},
booktitle = {Proceedings of the Artificial Intelligence and the Simulation of Behaviour Symposium on Computing and Philosophy}
}

@article{jordanous2012standardised,
  title={A Standardised Procedure for Evaluating Creative Systems: Computational Creativity Evaluation Based on What it is to be Creative},
  author={Jordanous, Anna},
  journal={Cognitive Computation},
  volume={4},
  pages={246--279},
  year={2012},
  publisher={Springer},
  doi={10.1007/s12559-012-9156-1}
}

@inproceedings{ventura2016mere,
  title={Mere generation: Essential barometer or dated concept},
  author={Ventura, Dan},
  booktitle={Proceedings of the International Conference on Computational Creativity},
  pages={17--24},
  year={2016},
  organization={Sony CSL}
}

@inproceedings{ackerman2017teaching,
title={Teaching Computational Creativity},
author={Ackerman, Margareta and Goel, Ashok and Johnson, Colin G. and Jordanous, Anna and Le√≥n, Carlos and P√©rez y P√©rez, Rafael and Toivonen, Hannu and Ventura, Dan},
booktitle={Proceedings of the International Conference on Computational Creativity},
pages={9--16},
year={2017},
organization={Association for Computational Creativity}
}


@article{wiggins2006,
author = {Wiggins, Geraint A.},
journal={Knowlege-Based Systems},
title = {A Preliminary Framework for Description, Analysis and Comparison of Creative Systems},
year = {2006},
volume = {19},
pages = {449‚Äì458}
}

@inproceedings{lamb2015,
  author={Carolyn Lamb and Daniel G. Brown and Charles Clarke},
  booktitle={Proceedings of the International Conference on Computational Creativity},
  title={Human Competence in Creativity Evaluation},
  year={2015},
  pages={102‚Äì109},
  publisher = {Brigham Young University}
 }
@inproceedings{bown2014,
  author={Oliver Bown},
  booktitle={Proceedings of the International Conference on Computational Creativity},
  title={Empirically Grounding the Evaluation of Creative Systems: Incorporating Interaction Design},
  year={2014},
  pages={112‚Äì119}
 }

@inproceedings{pease2011,
  author={Pease, A. and Colton, S.},
  booktitle={Proceedings of the International Conference on Computational Creativity}, 
  title={Computational Creativity Theory: Inspirations behind the {FACE} and the {IDEA} models}, 
  year={2011},
  pages={72‚Äì77}
 }

@inproceedings{colton2011,
  author={Colton, S. and Charnley, J. and Pease, A.},
  booktitle={Proceedings of the International Conference on Computational Creativity}, 
  title={ Computational Creativity Theory: The FACE and IDEA Descriptive Models}, 
  year={2011},
  pages={90‚Äì95}
 }

@inproceedings{carnovalini2021,
  author={Filippo Carnovalini and Nicholas Harley and Steven T. Homer and Antonio Roda and Geraint A. Wiggins},
  booktitle={Proceedings of the International Conference on Computational Creativity}, 
  title={Meta-evaluating Quantitative Internal Evaluation: A Practical Approach for Developers}, 
  year={2021},
  pages={213‚Äì217},
  publisher = {Association for Computational Creativity}
 }

@inproceedings{peeperkorn2023,
  author={Max Peeperkorn and Dan Brown and Anna Jordanous},
  booktitle={Proceedings of the International Conference on Computational Creativity}, 
  title={On Characterizations of Large Language Models and Creativity Evaluation}, 
  year={2023},
  pages={143‚Äì147},
  publisher = {Association for Computational Creativity}
 }

@inproceedings{spendlove2023,
  author={Spendlove, Brad and Ventura, Dan},
  booktitle={Proceedings of the International Conference on Computational Creativity}, 
  title={A Constraint-centric Accounting of Some Aspects of Creativity}, 
  year={2023},
  pages={332‚Äì336},
  publisher = {Association for Computational Creativity}
 }

@inproceedings{fragapane2021,
  author={Fragapane, Vincenzo and Standl, Bernhard},
  booktitle={Proceedings of the IEEE Global Engineering Education Conference}, 
  title={Work in Progress: Creative Coding and Computer Science Education ‚Äì From Approach to Concept}, 
  year={2021},
  pages={1233‚Äì1236}
 }

@inproceedings{yee-king2024,
author = {Yee-King, M. and Fiorucci, A. and  d'Inverno, M.},
booktitle={Proceedings of The International Conference on AI and Musical Creativity},
title = {Designing an {AI}-creativity music course},
year = {2024}
}
@article{kakavas2019,
author = {Kakavas, P.},
journal={Formamente},
title = {Computational thinking and Creativity in {K-6} Education},
year = {2019},
volume = {14},
number = {2},
pages = {93--108}
}

@inproceedings{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in Neural Information Processing Systems},
  volume={26},
  pages={3111--3119},
  year={2013}
}

@article{orkphol2019word,
  title={Word sense disambiguation using cosine similarity collaborates with {Word2vec} and {WordNet}},
  author={Orkphol, Korawit and Yang, Wu},
  journal={Future Internet},
  volume={11},
  number={5},
  pages={114},
  year={2019},
  publisher={MDPI}
}

@misc{codenames2015rules,
  title        = {Codenames Rules},
  author       = {{Czech Games Edition}},
  year         = {2015},
  organization = {Czech Games Edition},
  howpublished          = {https://czechgames.com/files/rules/codenames-rules-en.pdf},
  note         = {Official rulebook for the Codenames board game}
}

  @book{boden92,
author = {Margaret Boden},
title = {The Creative Mind},
publisher = {Abacus},
year = {1992},
}



@misc{UCI,
 author = "A. Asuncion and D.J. Newman",
 title = "{UCI} Machine Learning Repository", 
 note = "http://www.ics.uci.edu/~mlearn/MLRepository.html",
 howpublished = "University of California, Irvine, School of Information and Computer Sciences"
}


@inproceedings{veale07,
 author = {Tony Veale and Yanfen Hao},
 title = {Comprehending and Generating Apt Metaphors: A Web-driven, Case-based Approach to Figurative Language},
 booktitle = {Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence (AAAI-07)},
 year = {2007},
 pages = {1471--1476},
 publisher = {AAAI Press},
 address = {Vancouver, British Columbia},
 }

@article{lyu04,
 author = {Siwei Lyu and Daniel Rockmore and Hany Farid},
 title = {A Digital Technique for Art Authentication},
 journal = {Proceedings of the National Academy of Sciences},
 volume = {101},
 number = {49},
 year = {2004},
 pages = {17006--17010},
 }

@incollection{Woods81,
 author =      "Woods, W.A.",
 title =       "Procedural semantics as a theory of meaning",
 booktitle =   "Elements of Discourse Understanding",
 publisher =   "Cambridge University Press",
 address =     "Cambridge, UK",
 year =        1981,
 pages =       "300--334"
}

@Book{Ruch07,
 title =       {The Sense of Humor: Explorations of a Personality
Characteristic},
 publisher =   {Mouton de Gruyter},
 year =        2007,
 series =      {Mouton Select},
 address =     {Berlin}
}

@techreport{OZ,
 AUTHOR =      {Mark Kantrowitz},
 TITLE =       {Natural Language Text Generation in the {OZ}
                 Interactive Fiction Project},
 YEAR =        {1990},
 INSTITUTION =         {School of Computer Science, Carnegie Mellon
                 University},
 ADDRESS =     {Pittsburgh, PA},
 NUMBER =      {CMU-CS-90-158},
 TYPE =        {Technical Report}
}





@inproceedings{mikolov2013word2vec,
 author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {3111--3119},
 title = {Distributed Representations of Words and Phrases and their Compositionality},
 url = {https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf},
 year = {2013}
}

@misc{touvron2023llama2openfoundation,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      howpublished={https://arxiv.org/abs/2307.09288}, 
}


@misc{gpt-4o,
  author = {OpenAI},
  title = {{ChatGPT (GPT-4o version)}},
  year = {2024},
  howpublished = {\url{https://chat.openai.com}},
  note = {Accessed 2025-2-14},
}


@inproceedings{
yang2024opro,
title={Large Language Models as Optimizers},
author={Chengrun Yang and Xuezhi Wang and Yifeng Lu and Hanxiao Liu and Quoc V Le and Denny Zhou and Xinyun Chen},
booktitle={Proceedings of the International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=Bb4VGOWELI}
}

@article{liu2023prompt,
author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
title = {Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3560815},
doi = {10.1145/3560815},
abstract = {This article surveys and organizes research works in a new paradigm in natural language processing, which we dub ‚Äúprompt-based learning.‚Äù Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x‚Ä≤ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string xÃÇ, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.,&nbsp;the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website  including constantly updated survey and paperlist.},
journal = {ACM Computing Surveys},
articleno = {195},
pages = {Article 195},
keywords = {Pre-trained language models, prompting}
}

@inproceedings{brown2020gpt3,
author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
title = {Language models are few-shot learners},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
booktitle = {Advances in Neural Information Processing Systems},
articleno = {159},
pages = {Pages 1877--1901},
}

@inproceedings{li-liang-2021-prefix,
    title = "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
    author = "Li, Xiang Lisa  and
      Liang, Percy",
    booktitle = "Proceedings of the Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing",
    year = "2021",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.353",
    doi = "10.18653/v1/2021.acl-long.353",
    pages = "4582--4597",
    abstract = "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were {``}virtual tokens{''}. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1{\%} of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.",
}

@inproceedings{deng2022rlprompt,
    title = "{RLP}rompt: Optimizing Discrete Text Prompts with Reinforcement Learning",
    author = "Deng, Mingkai  and
      Wang, Jianyu  and
      Hsieh, Cheng-Ping  and
      Wang, Yihan  and
      Guo, Han  and
      Shu, Tianmin  and
      Song, Meng  and
      Xing, Eric  and
      Hu, Zhiting",
    booktitle = "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
    year = "2022",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.222",
    doi = "10.18653/v1/2022.emnlp-main.222",
    pages = "3369--3391",
    abstract = "Prompting has shown impressive success in enabling large pre-trained language models (LMs) to perform diverse NLP tasks, especially with only few downstream data. Automatically finding the optimal prompt for each task, however, is challenging. Most existing work resorts to tuning *soft* prompts (e.g., embeddings) which fall short of interpretability, reusability across LMs, and applicability when gradients are not accessible. *Discrete* prompts, on the other hand, are difficult to optimize, and are often created by {``}enumeration (e.g., paraphrasing)-then-selection{''} heuristics that do not explore the prompt space systematically. This paper proposes RLPrompt, an efficient discrete prompt optimization approach with reinforcement learning (RL). RLPrompt formulates a parameter-efficient policy network that generates the optimized discrete prompt after training with reward. To harness the complex and stochastic reward signals from the large LM environment, we incorporate effective reward stabilization that substantially enhances training efficiency. RLPrompt is flexibly applicable to different types of LMs, such as masked (e.g., BERT) and left-to-right models (e.g., GPTs), for both classification and generation tasks. Experiments on few-shot classification and unsupervised text style transfer show superior performance over a wide range of existing fine-tuning or prompting methods. Interestingly, the resulting optimized prompts are often ungrammatical gibberish text; and surprisingly, those gibberish prompts are transferrable between different LMs to retain significant performance, indicating that LM prompting may not follow human language patterns.",
}

@article{cobbe2023gsm8k,
  author       = {Karl Cobbe and
                  Vineet Kosaraju and
                  Mohammad Bavarian and
                  Mark Chen and
                  Heewoo Jun and
                  Lukasz Kaiser and
                  Matthias Plappert and
                  Jerry Tworek and
                  Jacob Hilton and
                  Reiichiro Nakano and
                  Christopher Hesse and
                  John Schulman},
  title        = {Training Verifiers to Solve Math Word Problems},
  journal      = {CoRR},
  volume       = {abs/2110.14168},
  year         = {2021},
  url          = {https://arxiv.org/abs/2110.14168},
  eprinttype    = {arXiv},
  eprint       = {2110.14168},
  timestamp    = {Mon, 12 Jun 2023 08:23:44 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2110-14168.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{suzgun-etal-2023-challenging,
    title = "Challenging {BIG}-Bench Tasks and Whether Chain-of-Thought Can Solve Them",
    author = {Suzgun, Mirac  and
      Scales, Nathan  and
      Sch{\"a}rli, Nathanael  and
      Gehrmann, Sebastian  and
      Tay, Yi  and
      Chung, Hyung Won  and
      Chowdhery, Aakanksha  and
      Le, Quoc  and
      Chi, Ed  and
      Zhou, Denny  and
      Wei, Jason},
    booktitle = "Findings of the Association for Computational Linguistics",
    year = "2023",
    publisher = "Association for Computational Linguistics",
    pages = "13003--13051",
    abstract = "BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65{\%} of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models? In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the tasks for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves."
}

@inproceedings{wei2022cot,
author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed H. and Le, Quoc V. and Zhou, Denny},
title = {Chain-of-thought prompting elicits reasoning in large language models},
year = {2022},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
abstract = {We explore how generating a chain of thought‚Äîa series of intermediate reasoning steps‚Äîsignificantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-of-thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting.Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
booktitle = {Advances in Neural Information Processing Systems},
articleno = {1800},
pages = { 24824--24837},
}

@misc{o1,
  author       = {OpenAI},
  title        = {Learning to Reason with {LLMs}},
  year = {2024},
  howpublished = {Blog post},
  url          = {https://openai.com/index/learning-to-reason-with-llms/},
  note         = {Accessed: February 26, 2025}
}

@inproceedings{veale2024symbolic,
  title={From Symbolic Caterpillars to Stochastic Butterflies: Case Studies in Re-Implementing Creative Systems with {LLMs}},
  author={Veale, Tony},
  booktitle={Proceedings of the International Conference on Computational Creativity},
  publisher={Association for Computational Creativity},
pages={236--244},
  year={2024}
}

@article{lazovsky2025prompt,
author = {Sasson Lazovsky, Gal and Raz, Tuval and Kenett, Yoed N.},
title = {The Art of Creative Inquiry‚ÄîFrom Question Asking to Prompt Engineering},
journal = {The Journal of Creative Behavior},
volume = {59},
number = {1},
pages = {e671},
keywords = {prompt engineering, question asking, creativity, large language models},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/jocb.671},
abstract = {ABSTRACT As¬†artificial intelligence and natural language processing methods rapidly develop, communication plays a pivotal role in every-day interactions. In this theoretical paper, we explore the overlap and commonalities between question-asking and prompt engineering. While seemingly distinct, these processes share a common foundation in essential skills like creativity, critical thinking, and cognitive flexibility. We contend that prompt engineering, the art of crafting cues for language models, and question-asking, the skill of formulating inquiries, form a symbiotic relationship. Delving into question complexity through Bloom's taxonomy and diverse types of questions, we propose strategies for not only efficient but also engaging prompt design. Our theoretical contribution emphasizes the dynamic role of creativity in both processes, offering intriguing perspectives on human‚Äìmachine interactions and advancing our understanding of language models and communication skills.},
year = {2025}
}

@misc{openai_prompt_engineering,
  author = {OpenAI},
  title = {Prompt Engineering},
  howpublished = {\url{https://platform.openai.com/docs/guides/prompt-engineering}},
  year = {2025},
  note = {Accessed: February 27, 2025}
}

@misc{deepseek2025r1,
      title={{DeepSeek-R1}: Incentivizing Reasoning Capability in {LLM}s via Reinforcement Learning}, 
      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      howpublished={https://arxiv.org/abs/2501.12948}, 
}

@misc{Glatch2025flash,
  author = {Sean Glatch},
  title = {How to Write Flash Fiction Stories},
  howpublished = {https://writers.com/how-to-write-flash-fiction},
  publisher = {Writers.com},
  year = {2024},
  urldate = {2025-02-27}
}

@inproceedings{morain2023language,
author = {Robert Morain and Branden Kinghorn and Dan Ventura},
title = {Are language models unsupervised multi-domain {CC} systems?},
booktitle = {Proceedings of the International Conference on Computational Creativity},
pages = {39--43},
year = {2023},
publisher = {Association for Computational Creativity}
}

@inproceedings{calderwood2020novelists,
 author  = {Alex Calderwood and Vivian Qiu and Katy Ilonka Gero and Lydia B. Chilton},
 title = {How Novelists Use Generative Language Models: An Exploratory User Study},
 booktitle = {Joint Proceedings of the Workshops on Human-AI Co-Creation with Generative Models and User-Aware Conversational Agents},
 publisher = {CEUR-WS},
 year = {2020}, 
}

@inproceedings{sawicki2023power,
  title={On the power of special-purpose {GPT} models to create and evaluate new poetry in old styles},
  booktitle={Proceedings of the International Conference on Computational Creativity},
  author={Sawicki, Piotr and Grzes, Marek and G{\'o}es, Luis Fabricio and Brown, Dan and Peeperkorn, Max and Khatun, Aisha and Paraskevopoulou, Simona},
  pages={10--19},
  year={2023},
  publisher={Association for Computational Creativity},
}

@inproceedings{toplyn2021witscript,
  author    = {Joe Toplyn},
  title     = {Witscript: {A} System for Generating Improvised Jokes in a Conversation},
  booktitle = {Proceedings of the International Conference on Computational Creativity},
  pages     = {22--31},
  year      = {2021},
  timestamp = {Fri, 17 Dec 2021 11:39:35 +0100},
  biburl    = {https://dblp.org/rec/conf/icccrea/Toplyn21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  publisher={Association for Computational Creativity},
}

@inproceedings{peeperkorn2024temperature,
  title={Is temperature the creativity parameter of large language models?},
  author={Peeperkorn, Max and Kouwenhoven, Tom and Brown, Dan and Jordanous, Anna},
  booktitle = {Proceedings of the International Conference on Computational Creativity},
  year={2024},
  publisher={Association for Computational Creativity},
}

@inproceedings{shin2020autoprompt,
    title = "{AutoPrompt}: Eliciting Knowledge from {L}anguage Models with Automatically Generated Prompts",
    author = "Shin, Taylor  and
      Razeghi, Yasaman  and
      Logan IV, Robert L.  and
      Wallace, Eric  and
      Singh, Sameer",
    booktitle = "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
    year = "2020",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.346",
    pages = "4222--4235",
    abstract = "The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.",
}

@inproceedings{colton2008tripod,
  author    = {Simon Colton},
  title     = {Creativity Versus the Perception of Creativity in Computational Systems},
  booktitle = {{AAAI} Spring Symposium: Creative Intelligent Systems},
  pages     = {14--20},
  volume    = {8},
  publisher = {{AAAI}},
  year      = {2008},
  url       = {http://www.aaai.org/Library/Symposia/Spring/2008/ss08-03-003.php},
  timestamp = {Fri, 17 Feb 2012 14:29:47 +0100},
  biburl    = {https://dblp.org/rec/conf/aaaiss/Colton08.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{promptbase,
  title = {PromptBase: Marketplace for {AI} Prompts},
  author = {PromptBase},
  howpublished = {https://promptbase.com/},
  year = {2025},
  note = {Accessed: March 1, 2025}
}

@inproceedings{reimers-2019-sentence-bert,
  title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
  author = "Reimers, Nils and Gurevych, Iryna",
  booktitle = "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
  year = "2019",
  publisher = "Association for Computational Linguistics",
}

@misc{mcinnes2020umap,
      title={UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction}, 
      author={Leland McInnes and John Healy and James Melville},
      year={2020},
      eprint={1802.03426},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      howpublished={https://arxiv.org/abs/1802.03426}, 
}

@inproceedings{spendlove2020humans,
  title={Humans in the Black Box: A New Paradigm for Evaluating the Design of Creative Systems.},
  author={Spendlove, Brad and Ventura, Dan},
  booktitle = {Proceedings of the International Conference on Computational Creativity},
  pages={311--318},
  year={2020},
  publisher={Association for Computational Creativity},
}

@misc{UCI,
 author = "A. Asuncion and D.J. Newman",
 title = "{UCI} Machine Learning Repository", 
 note = "http://www.ics.uci.edu/~mlearn/MLRepository.html",
 howpublished = "University of California, Irvine, School of Information and Computer Sciences"
}


@inproceedings{veale07,
 author = {Tony Veale and Yanfen Hao},
 title = {Comprehending and Generating Apt Metaphors: A Web-driven, Case-based Approach to Figurative Language},
 booktitle = {Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence (AAAI-07)},
 year = {2007},
 pages = {1471--1476},
 publisher = {AAAI Press},
 address = {Vancouver, British Columbia},
 }

@article{lyu04,
 author = {Siwei Lyu and Daniel Rockmore and Hany Farid},
 title = {A Digital Technique for Art Authentication},
 journal = {Proceedings of the National Academy of Sciences},
 volume = {101},
 number = {49},
 year = {2004},
 pages = {17006--17010},
 }

@incollection{Woods81,
 author =      "Woods, W.A.",
 title =       "Procedural semantics as a theory of meaning",
 booktitle =   "Elements of Discourse Understanding",
 publisher =   "Cambridge University Press",
 address =     "Cambridge, UK",
 year =        1981,
 pages =       "300--334"
}

@Book{Ruch07,
 title =       {The Sense of Humor: Explorations of a Personality
Characteristic},
 publisher =   {Mouton de Gruyter},
 year =        2007,
 series =      {Mouton Select},
 address =     {Berlin}
}

@techreport{OZ,
 AUTHOR =      {Mark Kantrowitz},
 TITLE =       {Natural Language Text Generation in the {OZ}
                 Interactive Fiction Project},
 YEAR =        {1990},
 INSTITUTION =         {School of Computer Science, Carnegie Mellon
                 University},
 ADDRESS =     {Pittsburgh, PA},
 NUMBER =      {CMU-CS-90-158},
 TYPE =        {Technical Report}
}


@phdthesis{perez1999mexica,
  title={MEXICA: a computer model of creativity in writing.},
  author={P{\'e}rez y P{\'e}rez, Rafael},
  year={1999},
  school={University of Sussex}
}

@misc{ollama2024,
  author       = {Ollama},
  title        = {ollama.com},
  year         = {2024},
  url          = {https://ollama.com},
  note         = {Accessed: 2025-04-23}
}

@inproceedings{VallsVargas2017TowardsAE,
  title={Towards Automatically Extracting Story Graphs from Natural Language Stories},
  author={Josep Valls-Vargas and Jichen Zhu and Santiago Onta{\~n}{\'o}n},
  booktitle={AAAI Workshops},
  year={2017},
}

@inproceedings{chambers-jurafsky-2008-unsupervised,
    title = "Unsupervised Learning of Narrative Event Chains",
    author = "Chambers, Nathanael  and
      Jurafsky, Dan",
    booktitle = "Proceedings of the Anniversary Meeting of the Association for Computational Linguistics",
    year = "2008",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P08-1090/",
    pages = "789--797"
}

@incollection{steel1922goldilocks,
  author    = {Flora Annie Steel},
  title     = {The Story of the Three Bears},
  booktitle = {English Fairy Tales},
  publisher = {Macmillan and Co.},
  year      = {1922},
  url       = {https://www.gutenberg.org/ebooks/17034}
}

@incollection{steel1918jack,
  author    = {Flora Annie Steel},
  title     = {Jack and the Beanstalk},
  booktitle = {English Fairy Tales},
  publisher = {Macmillan and Co., Limited},
  address   = {London},
  year      = {1918},
  pages     = {136--153},
  url       = {https://www.gutenberg.org/files/17034/17034-h/17034-h.htm}
}

@incollection{grimm1812hansel,
  author    = {Jacob Grimm and Wilhelm Grimm},
  title     = {H√§nsel und Gretel},
  booktitle = {Kinder- und Hausm√§rchen},
  volume    = {1},
  publisher = {Realschulbuchhandlung},
  address   = {Berlin},
  year      = {1812},
  note      = {KHM 15},
  language  = {German},
  url       = {https://en.wikipedia.org/wiki/Hansel_and_Gretel}
}

@incollection{perrault1923little,
  author    = {Charles Perrault},
  title     = {Little Red Riding-Hood},
  booktitle = {Tales of Past Times Written for Children},
  publisher = {E.P. Dutton and Co.},
  address   = {New York},
  year      = {1923},
  pages     = {6--8},
  url       = {https://www.colorado.edu/projects/fairy-tales/charles-perrault-little-red-riding-hood}
}

@book{perez2023narrative,
  title={An introduction to narrative generators: how computers create works of fiction},
  author={P{\'e}rez y P{\'e}rez, Rafael and Sharples, Mike},
  year={2023},
  publisher={Oxford University Press}
}

@misc{rumi,
title={Multimodal paralinguistic prompting for large language models},
author={Weiwei Yang and Spencer Fowers and David Tittsworth and Amber Hoak and Thiago Vallin Spina and Kate Lytvynets and Christopher O'Dowd and Andrea Britto Mattos Lima and Whitney Hudson and Ben Cutler and Prachi Patel and Robert Morain},
publisher={Microsoft Research},
howpublished={\url{https://www.microsoft.com/en-us/research/project/project-rumi/}},
note={Accessed 2024-2-13},
year={2023}
}

@article{liu2021prompt,
  author    = {Pengfei Liu and
               Weizhe Yuan and
               Jinlan Fu and
               Zhengbao Jiang and
               Hiroaki Hayashi and
               Graham Neubig},
  title     = {Pre-train, Prompt, and Predict: {A} Systematic Survey of Prompting
               Methods in Natural Language Processing},
  journal   = {CoRR},
  volume    = {abs/2107.13586},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.13586},
  eprinttype = {arXiv},
  eprint    = {2107.13586},
  timestamp = {Tue, 03 Aug 2021 14:53:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-13586.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{wu2022idpg,
    title = "{IDPG}: An Instance-Dependent Prompt Generation Method",
    author = "Wu, Zhuofeng  and
      Wang, Sinong  and
      Gu, Jiatao  and
      Hou, Rui  and
      Dong, Yuxiao  and
      Vydiswaran, V.G.Vinod  and
      Ma, Hao",
    booktitle = "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    year = "2022",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.403",
    pages = "5507--5521",
    abstract = "Prompt tuning is a new, efficient NLP transfer learning paradigm that adds a task-specific prompt in each input instance during the model training stage. It freezes the pre-trained language model and only optimizes a few task-specific prompts. In this paper, we propose a conditional prompt generation method to generate prompts for each input instance, referred to as the Instance-Dependent Prompt Generation (IDPG). Unlike traditional prompt tuning methods that use a fixed prompt, IDPG introduces a lightweight and trainable component to generate prompts based on each input sentence. Extensive experiments on ten natural language understanding (NLU) tasks show that the proposed strategy consistently outperforms various prompt tuning baselines and is on par with other efficient transfer learning methods such as Compacter while tuning far fewer model parameters.",
}

@article{schulman2017ppo,
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time. },
  added-at = {2019-12-16T18:31:56.000+0100},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  biburl = {https://www.bibsonomy.org/bibtex/24bbcce6aa1c42ae7f61ef8cf5475aa85/lanteunis},
  ee = {http://arxiv.org/abs/1707.06347},
  interhash = {f57ff463a90dbafb77d55a25aea8355c},
  intrahash = {4bbcce6aa1c42ae7f61ef8cf5475aa85},
  journal = {CoRR},
  keywords = {DRLAlgoComparison ppo reinforcement_learning},
  timestamp = {2019-12-18T21:15:59.000+0100},
  title = {Proximal Policy Optimization Algorithms.},
  url = {https://arxiv.org/abs/1707.06347},
  volume = {abs/1707.06347},
  year = 2017
}


@inproceedings{bengio2000neural,
 author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {MIT Press},
 title = {A Neural Probabilistic Language Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf},
 volume = {13},
 year = {2000}
}

@inproceedings{ouyang2022rlhf,
 author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F and Leike, Jan and Lowe, Ryan},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {27730--27744},
 publisher = {Curran Associates, Inc.},
 title = {Training language models to follow instructions with human feedback},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@inproceedings{dathathri2020plug,
title={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},
author={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=H1edEyBKDS}
}

@article{keskar2019ctrl,
  author       = {Nitish Shirish Keskar and
                  Bryan McCann and
                  Lav R. Varshney and
                  Caiming Xiong and
                  Richard Socher},
  title        = {{CTRL:} {A} Conditional Transformer Language Model for Controllable
                  Generation},
  journal      = {CoRR},
  volume       = {abs/1909.05858},
  year         = {2019},
  url          = {http://arxiv.org/abs/1909.05858},
  eprinttype    = {arXiv},
  eprint       = {1909.05858},
  timestamp    = {Wed, 18 Sep 2019 10:38:36 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1909-05858.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{krause2021gedi,
    title = "{G}e{D}i: Generative Discriminator Guided Sequence Generation",
    author = "Krause, Ben  and
      Gotmare, Akhilesh Deepak  and
      McCann, Bryan  and
      Keskar, Nitish Shirish  and
      Joty, Shafiq  and
      Socher, Richard  and
      Rajani, Nazneen Fatema",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP",
    year = "2021",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.424",
    pages = "4929--4952",
    abstract = "",
}
@inproceedings{liu2021dexpert,
    title = "{DE}xperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts",
    author = "Liu, Alisa  and
      Sap, Maarten  and
      Lu, Ximing  and
      Swayamdipta, Swabha  and
      Bhagavatula, Chandra  and
      Smith, Noah A.  and
      Choi, Yejin",
    booktitle = "Proceedings of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing",
    volume = 1,
    year = "2021",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.522",
    pages = "6691--6706",
    abstract = "Despite recent advances in natural language generation, it remains challenging to control attributes of generated text. We propose DExperts: Decoding-time Experts, a decoding-time method for controlled text generation that combines a pretrained language model with {``}expert{''} LMs and/or {``}anti-expert{''} LMs in a product of experts. Intuitively, under the ensemble, tokens only get high probability if they are considered likely by the experts, and unlikely by the anti-experts. We apply DExperts to language detoxification and sentiment-controlled generation, where we outperform existing controllable generation methods on both automatic and human evaluations. Moreover, because DExperts operates only on the output of the pretrained LM, it is effective with (anti-)experts of smaller size, including when operating on GPT-3. Our work highlights the promise of tuning small LMs on text with (un)desirable attributes for efficient decoding-time steering.",
}
@inproceedings{zhang2022discup,
    title = "{D}is{C}up: Discriminator Cooperative Unlikelihood Prompt-tuning for Controllable Text Generation",
    author = "Zhang, Hanqing  and
      Song, Dawei",
    booktitle = "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
    year = "2022",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.223",
    pages = "3392--3406",
    abstract = "Prompt learning with immensely large Casual Language Models (CLMs) has been shown promising for attribute-controllable text generation (CTG). However, vanilla prompt tuning tends to imitate training corpus characteristics beyond the control attributes, resulting in a poor generalization ability. Moreover, it is less able to capture the relationship between different attributes, further limiting the control performance. In this paper, we propose a new CTG approach, namely DisCup, which incorporates the attribute knowledge of discriminator to optimize the control-prompts, steering a frozen CLM to produce attribute-specific texts. Specifically, the frozen CLM model, capable of producing multitudinous texts, is first used to generate the next-token candidates based on the context, so as to ensure the diversity of tokens to be predicted. Then, we leverage an attribute-discriminator to select desired/undesired tokens from those candidates, providing the inter-attribute knowledge. Finally, we bridge the above two traits by an unlikelihood objective for prompt-tuning. Extensive experimental results show that DisCup can achieve a new state-of-the-art control performance while maintaining an efficient and high-quality text generation, only relying on around 10 virtual tokens.",
}




@article{zhang2023rmt,
  title={Controllable Text Generation with Residual Memory Transformer},
  author={Hanqing Zhang and Sun Si and Haiming Wu and Dawei Song},
  year="2023",
  journal={CoRR},
  volume={abs/2309.16231},
  url={https://arxiv.org/abs/2309.16231},
  eprinttype={arXiv},
  eprint={2309.16231},
  primaryClass={cs.CL}
}

@misc{vonwerra2022trl,
  author = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang},
  title = {TRL: Transformer Reinforcement Learning},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/trl}}
}

@article{zhang2023survey,
author = {Zhang, Hanqing and Song, Haolin and Li, Shaoyu and Zhou, Ming and Song, Dawei},
title = {A Survey of Controllable Text Generation Using Transformer-based Pre-trained Language Models},
year = {2023},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
volume = {56},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3617680},
abstract = {Controllable Text Generation (CTG) is an emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used Transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the limited level of interpretability of deep neural networks, the controllability of these methods needs to be guaranteed. To this end, controllable text generation using Transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the past 3 to 4 years, targeting different CTG tasks that require different types of controlled constraints. In this article, we present a systematic critical review on the common tasks, main approaches, and evaluation methods in this area. Finally, we discuss the challenges that the field is facing, and put forward various promising future directions. To the best of our knowledge, this is the first survey article to summarize the state-of-the-art CTG techniques from the perspective of Transformer-based PLMs. We hope it can help researchers and practitioners in the related fields to quickly track the academic and technological frontier, providing them with a landscape of the area and a roadmap for future research.},
journal = {Association for Computing Machinery Computing Surveys},
articleno = {64},
numpages = {37},
keywords = {systematic review, controllability, Transformer, pre-trained language models, Controllable text generation}
}

@article{ziegler2019preferences,
  author       = {Daniel M. Ziegler and
                  Nisan Stiennon and
                  Jeffrey Wu and
                  Tom B. Brown and
                  Alec Radford and
                  Dario Amodei and
                  Paul F. Christiano and
                  Geoffrey Irving},
  title        = {Fine-Tuning Language Models from Human Preferences},
  journal      = {CoRR},
  volume       = {abs/1909.08593},
  year         = {2019},
  url          = {http://arxiv.org/abs/1909.08593},
  eprinttype    = {arXiv},
  eprint       = {1909.08593},
  timestamp    = {Thu, 01 Apr 2021 19:06:51 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1909-08593.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS {nguyen2017ppgn,
author = {A. Nguyen and J. Clune and Y. Bengio and A. Dosovitskiy and J. Yosinski},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Plug \& Play Generative Networks: Conditional Iterative Generation of Images in Latent Space},
year = {2017},
volume = {},
issn = {1063-6919},
pages = {3510-3520},
abstract = {Generating high-resolution, photo-realistic images has been a long-standing goal in machine learning. Recently, Nguyen et al. [37] showed one interesting way to synthesize novel images by performing gradient ascent in the latent space of a generator network to maximize the activations of one or multiple neurons in a separate classifier network. In this paper we extend this method by introducing an additional prior on the latent code, improving both sample quality and sample diversity, leading to a state-of-the-art generative model that produces high quality images at higher resolutions (227 √ó 227) than previous generative models, and does so for all 1000 ImageNet categories. In addition, we provide a unified probabilistic interpretation of related activation maximization methods and call the general class of models Plug and Play Generative Networks. PPGNs are composed of 1) a generator network G that is capable of drawing a wide range of image types and 2) a replaceable condition network C that tells the generator what to draw. We demonstrate the generation of images conditioned on a class (when C is an ImageNet or MIT Places classification network) and also conditioned on a caption (when C is an image captioning network). Our method also improves the state of the art of Multifaceted Feature Visualization [40], which generates the set of synthetic inputs that activate a neuron in order to better understand how deep neural networks operate. Finally, we show that our model performs reasonably well at the task of image inpainting. While image models are used in this paper, the approach is modality-agnostic and can be applied to many types of data.},
keywords = {neurons;generators;training;feature extraction;plugs;image resolution;probabilistic logic},
doi = {10.1109/CVPR.2017.374},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2017.374},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
}

@inproceedings{
Welleck2020Neural,
title={Neural Text Generation With Unlikelihood Training},
author={Sean Welleck and Ilia Kulikov and Stephen Roller and Emily Dinan and Kyunghyun Cho and Jason Weston},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SJeYe0NtvH}
}

@inproceedings{li2021prefix,
    title = "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
    author = "Li, Xiang Lisa  and
      Liang, Percy",
    booktitle = "Proceedings of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing",
    year = "2021",
    volume = 1,
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.353",
    pages = "4582--4597",
    abstract = "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were {``}virtual tokens{''}. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1{\%} of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.",
}

@article{touvron2023llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth√©e Lacroix and Baptiste Rozi√®re and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      url = "https://arxiv.org/abs/2302.13971",
      primaryClass={cs.CL},
      journal={CoRR},
      volume={abs/2302.13971},
}

@inproceedings{sheng2019bias,
    title = "The Woman Worked as a Babysitter: On Biases in Language Generation",
    author = "Sheng, Emily  and
      Chang, Kai-Wei  and
      Natarajan, Premkumar  and
      Peng, Nanyun",
    booktitle = "Proceedings of the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1339",
    pages = "3407--3412",
    abstract = "We present a systematic study of biases in natural language generation (NLG) by analyzing text generated from prompts that contain mentions of different demographic groups. In this work, we introduce the notion of the regard towards a demographic, use the varying levels of regard towards different demographics as a defining metric for bias in NLG, and analyze the extent to which sentiment scores are a relevant proxy metric for regard. To this end, we collect strategically-generated text from language models and manually annotate the text with both sentiment and regard scores. Additionally, we build an automatic regard classifier through transfer learning, so that we can analyze biases in unseen text. Together, these methods reveal the extent of the biased nature of language model generations. Our analysis provides a study of biases in NLG, bias metrics and correlated human judgments, and empirical evidence on the usefulness of our annotated dataset.",
}

@inproceedings{gehman2020toxicity,
    title = "{R}eal{T}oxicity{P}rompts: Evaluating Neural Toxic Degeneration in Language Models",
    author = "Gehman, Samuel  and
      Gururangan, Suchin  and
      Sap, Maarten  and
      Choi, Yejin  and
      Smith, Noah A.",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP",
    year = "2020",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.301",
    pages = "3356--3369",
    abstract = "Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning {``}bad{''} words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.",
}

@inproceedings{socher2013sst5,
    title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    author = "Socher, Richard  and
      Perelygin, Alex  and
      Wu, Jean  and
      Chuang, Jason  and
      Manning, Christopher D.  and
      Ng, Andrew  and
      Potts, Christopher",
    booktitle = "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
    year = "2013",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D13-1170",
    pages = "1631--1642",
}

@inproceedings{maas2011imdb,
    title = "Learning Word Vectors for Sentiment Analysis",
    author = "Maas, Andrew L.  and
      Daly, Raymond E.  and
      Pham, Peter T.  and
      Huang, Dan  and
      Ng, Andrew Y.  and
      Potts, Christopher",
    booktitle = "Proceedings of the Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    year = "2011",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P11-1015",
    pages = "142--150",
}

@article{radford2019gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI Blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@misc{Gokaslan2019OpenWeb,  
	title={OpenWebText Corpus},
	author={Aaron Gokaslan and Vanya Cohen},
	howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}}, 
	year={2019}
}

@inproceedings{wolf2020huggingface,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    booktitle = "Proceedings of the Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    year = "2020",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.6",
    pages = "38--45",
    abstract = "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \url{https://github.com/huggingface/transformers}.",
}

@article{sanh2019distilbert,
  author       = {Victor Sanh and
                  Lysandre Debut and
                  Julien Chaumond and
                  Thomas Wolf},
  title        = {DistilBERT, a distilled version of {BERT:} smaller, faster, cheaper
                  and lighter},
  journal      = {CoRR},
  volume       = {abs/1910.01108},
  year         = {2019},
  url          = {http://arxiv.org/abs/1910.01108},
  eprinttype    = {arXiv},
  eprint       = {1910.01108},
  timestamp    = {Tue, 02 Jun 2020 12:48:59 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-01108.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{liu2019kalm,
  author    = {Angli Liu and
               Jingfei Du and
               Veselin Stoyanov},
  title     = {Knowledge-Augmented Language Model and Its Application to Unsupervised
               Named-Entity Recognition},
  booktitle = {Proceedings of the Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies},
  pages     = {1142--1150},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/n19-1117},
  timestamp = {Fri, 06 Aug 2021 00:41:31 +0200},
  biburl    = {https://dblp.org/rec/conf/naacl/LiuDS19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{peters2019knowbert,
  author    = {Matthew E. Peters and
               Mark Neumann and
               Robert L. Logan IV and
               Roy Schwartz an
               Vidur Joshi and
               Sameer Singh and
               Noah A. Smith},
  title     = {Knowledge Enhanced Contextual Word Representations},
  booktitle = {Proceedings of the Conference on Empirical Methods in Natural
               Language Processing and the International Joint Conference on
               Natural Language Processing},
  pages     = {43--54},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/D19-1005},
  timestamp = {Thu, 07 Apr 2022 09:14:07 +0200},
  biburl    = {https://dblp.org/rec/conf/emnlp/PetersNLSJSS19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{kingma2017adam,
  title={Adam: A Method for Stochastic Optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  year={2017},
  journal={CoRR},
  volume={abs/1412.6980},
  url={https://arxiv.org/abs/1412.6980},
  eprinttype={arXiv},
  eprint={1412.6980},
  primaryClass={cs.LG}
}

@misc{hanu2020detoxify,
  title={Detoxify},
  author={Hanu, Laura and the {Unitary team}},
  url={https://github.com/unitaryai/detoxify},

  year={2020}
}

@misc{meta2024llama3,
  title = {Introducing {Llama} 3.1: Our most capable models to date},
  author = {{Meta AI}},
  year = {2024},
  url = {https://ai.meta.com/blog/meta-llama-3-1/},
  note = {Accessed on October 15, 2024},
  organization = {Meta AI}
}

@inproceedings{qian-etal-2022-controllable,
    title = "Controllable Natural Language Generation with Contrastive Prefixes",
    author = "Qian, Jing  and
      Dong, Li  and
      Shen, Yelong  and
      Wei, Furu  and
      Chen, Weizhu",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.229",
    doi = "10.18653/v1/2022.findings-acl.229",
    pages = "2912--2924",
    abstract = "To guide the generation of large pretrained language models (LM), previous work has focused on directly fine-tuning the language model or utilizing an attribute discriminator. In this work, we propose a novel lightweight framework for controllable GPT2 generation, which utilizes a set of small attribute-specific vectors, called prefixes (Li and Liang, 2021), to steer natural language generation. Different from Li and Liang (2021), where each prefix is trained independently, we take the relationship among prefixes into consideration and train multiple prefixes simultaneously. We propose a novel supervised method and also an unsupervised method to train the prefixes for single-aspect control while the combination of these two methods can achieve multi-aspect control. Experimental results on both single-aspect and multi-aspect control show that our methods can guide generation towards the desired attributes while keeping high linguistic quality.",
}

@inproceedings{deng-etal-2022-rlprompt,
    title = "{RLP}rompt: Optimizing Discrete Text Prompts with Reinforcement Learning",
    author = "Deng, Mingkai  and
      Wang, Jianyu  and
      Hsieh, Cheng-Ping  and
      Wang, Yihan  and
      Guo, Han  and
      Shu, Tianmin  and
      Song, Meng  and
      Xing, Eric  and
      Hu, Zhiting",
    booktitle = "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
    year = "2022",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.222",
    doi = "10.18653/v1/2022.emnlp-main.222",
    pages = "3369--3391",
    abstract = "Prompting has shown impressive success in enabling large pre-trained language models (LMs) to perform diverse NLP tasks, especially with only few downstream data. Automatically finding the optimal prompt for each task, however, is challenging. Most existing work resorts to tuning *soft* prompts (e.g., embeddings) which fall short of interpretability, reusability across LMs, and applicability when gradients are not accessible. *Discrete* prompts, on the other hand, are difficult to optimize, and are often created by {``}enumeration (e.g., paraphrasing)-then-selection{''} heuristics that do not explore the prompt space systematically. This paper proposes RLPrompt, an efficient discrete prompt optimization approach with reinforcement learning (RL). RLPrompt formulates a parameter-efficient policy network that generates the optimized discrete prompt after training with reward. To harness the complex and stochastic reward signals from the large LM environment, we incorporate effective reward stabilization that substantially enhances training efficiency. RLPrompt is flexibly applicable to different types of LMs, such as masked (e.g., BERT) and left-to-right models (e.g., GPTs), for both classification and generation tasks. Experiments on few-shot classification and unsupervised text style transfer show superior performance over a wide range of existing fine-tuning or prompting methods. Interestingly, the resulting optimized prompts are often ungrammatical gibberish text; and surprisingly, those gibberish prompts are transferrable between different LMs to retain significant performance, indicating that LM prompting may not follow human language patterns.",
}

@techreport{meincke2025prompting,
  title        = {Prompting Science Report 1: Prompt Engineering is Complicated and Contingent},
  author       = {Meincke, Lennart and Mollick, Ethan R. and Mollick, Lilach and Shapiro, Dan},
  institution  = {SSRN},
  year         = {2025},
  url          = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5165270},
}

@article{Simonton2018DefiningCD,
  title={Defining Creativity: Don't We Also Need to Define What Is Not Creative?},
  author={Dean Keith Simonton},
  journal={Journal of Creative Behavior},
  year={2018},
  volume={52},
  pages={80-90},
  url={https://api.semanticscholar.org/CorpusID:152000109}
}

@book{Cskszentmihlyi1996CreativityFA,
  title={Creativity: Flow and the Psychology of Discovery and Invention},
  author={Mih{\'a}ly Cs{\'i}kszentmih{\'a}lyi},
  year={1996},
  publisher={HarperCollins},
  url={https://api.semanticscholar.org/CorpusID:142767919}
}

@article{Schmidhuber2022AnnotatedHO,
  title={Annotated History of Modern AI and Deep Learning},
  author={Juergen Schmidhuber},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.11279},
  url={https://api.semanticscholar.org/CorpusID:254974067}
}

@article{Gowda2023ArtificialII,
  title={Artificial Intelligence in the Modern Economy: Transformations, Applications, and Future Prospects},
  author={Kavitha R. Gowda},
  journal={Review of Artificial Intelligence in Education},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:261161697}
}

@inproceedings{ho2020denoising,
author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
title = {Denoising diffusion probabilistic models},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
booktitle = {Proceedings of the International Conference on Neural Information Processing Systems},
articleno = {574},
numpages = {12},
series = {NIPS '20}
}

@article{Agostinelli2023MusicLMGM,
  title={MusicLM: Generating Music From Text},
  author={Andrea Agostinelli and Timo I. Denk and Zal{\'a}n Borsos and Jesse Engel and Mauro Verzetti and Antoine Caillon and Qingqing Huang and Aren Jansen and Adam Roberts and Marco Tagliasacchi and Matthew Sharifi and Neil Zeghidour and Christian Havn{\o} Frank},
  journal={ArXiv},
  year={2023},
  volume={abs/2301.11325},
  url={https://api.semanticscholar.org/CorpusID:256274504}
}


@inproceedings{Ormazabal2022PoeLMAM,
  title={PoeLM: A Meter- and Rhyme-Controllable Language Model for Unsupervised Poetry Generation},
  author={Aitor Ormazabal and Mikel Artetxe and Manex Agirrezabal and Aitor Soroa Etxabe and Eneko Agirre},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:249018166}
}

@inproceedings{Fan2018HierarchicalNS,
  title={Hierarchical Neural Story Generation},
  author={Angela Fan and Mike Lewis and Yann Dauphin},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:44134226}
}

@inproceedings{Morris2023PositionLO,
  title={Position: Levels of AGI for Operationalizing Progress on the Path to AGI},
  author={Meredith Ringel Morris and Jascha Narain Sohl-Dickstein and Noah Fiedel and Tris Warkentin and Allan Dafoe and Aleksandra Faust and Cl{\'e}ment Farabet and Shane Legg},
  booktitle={International Conference on Machine Learning},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:265033463}
}

@article{Yu2023NaturalLR,
  title={Natural Language Reasoning, A Survey},
  author={Fei Yu and Hongbo Zhang and Benyou Wang},
  journal={ACM Computing Surveys},
  year={2023},
  volume={56},
  pages={1 - 39},
  url={https://api.semanticscholar.org/CorpusID:257766470}
}

@book{torrance1966tests,
  author    = {Ellis Paul Torrance},
  title     = {Torrance Tests of Creative Thinking},
  year      = {1966},
  publisher = {Personnel Press},
  address   = {Lexington, MA}
}

@book{guilford1967nature,
  author    = {J. P. Guilford},
  title     = {The Nature of Human Intelligence},
  year      = {1967},
  publisher = {McGraw-Hill},
  address   = {New York}
}

@article{mednick1968rat,
  author  = {Mednick, S. A.},
  title   = {The Remote Associates Test},
  journal = {The Journal of Creative Behavior},
  year    = {1968},
  volume  = {2},
  number  = {3},
  pages   = {213--214}
}
